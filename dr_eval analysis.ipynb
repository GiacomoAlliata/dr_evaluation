{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import textwrap as tw\n",
    "\n",
    "algo_names = [\"tsne\", \"umap\", \"trimap\", \"pacmap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Set to True if you want to load the narrative datasets (subjected to copyrigth), False for MNIST only\n",
    "NARRATIVES = False\n",
    "if NARRATIVES:\n",
    "    datasets_names = [\"rts\", \"pdl\", \"ioc\", \"mjf\"]\n",
    "else:\n",
    "    datasets_names = [\"mnist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "evaluation_results_df = []\n",
    "\n",
    "N_POINTS = 60000\n",
    "\n",
    "for dataset in datasets_names:\n",
    "    results = os.listdir(f\"results/{dataset}\")\n",
    "    results = [r for r in results if r.endswith(\".csv\")]\n",
    "    for result in results:\n",
    "        if str(N_POINTS) not in result:\n",
    "            continue\n",
    "        print(f\"Loading results/{dataset}/{result}\")\n",
    "        evaluation_results_df.append(pd.read_csv(f\"results/{dataset}/{result}\"))\n",
    "\n",
    "print(f\"Loaded {len(evaluation_results_df)} evaluation results\")\n",
    "evaluation_results_df = pd.concat(evaluation_results_df)\n",
    "evaluation_results_df.rename(columns={\"index\": \"dataset_algo\"}, inplace=True)\n",
    "\n",
    "evaluation_results_df[\"rta_local\"] = evaluation_results_df[\"rta\"].map(lambda x: eval(x)[\"local\"][0])\n",
    "evaluation_results_df[\"rta_global\"] = evaluation_results_df[\"rta\"].map(lambda x: eval(x)[\"global\"][0])\n",
    "evaluation_results_df[\"rta_all\"] = evaluation_results_df[\"rta\"].map(lambda x: eval(x)[\"all\"][0])\n",
    "evaluation_results_df.drop(columns=[\"rta\"], inplace=True)\n",
    "\n",
    "evaluation_results_df[\"dist_corr\"] = evaluation_results_df[\"dist_corr\"].map(lambda x: eval(x)[0])\n",
    "\n",
    "evaluation_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_metrics = [\"neighbor_kept_ratio\", \"global_score\", \"rta_local\", \"rta_global\", \"rta_all\", \"dist_corr\", \"run_time\"]\n",
    "\n",
    "scalar_metrics_df = evaluation_results_df[[\"dataset_algo\"] + scalar_metrics]\n",
    "scalar_metrics_df[\"run\"] = scalar_metrics_df[\"dataset_algo\"].apply(lambda x: x.split(\"_\")[2].replace(\"run\", \"\")).astype(int)\n",
    "scalar_metrics_df[\"dataset_algo\"] = scalar_metrics_df[\"dataset_algo\"].apply(lambda x: x.split(\"_\")[0] + \"_\" + x.split(\"_\")[1])\n",
    "scalar_metrics_df = scalar_metrics_df.groupby([\"dataset_algo\"])[scalar_metrics].agg([\"mean\", \"std\"])\n",
    "scalar_metrics_df.columns = ['_'.join(col).strip() for col in scalar_metrics_df.columns.values]\n",
    "scalar_metrics_df = scalar_metrics_df.apply(lambda row: [(row[f\"{metric}_mean\"], row[f\"{metric}_std\"]) for metric in scalar_metrics], axis=1, result_type='expand')\n",
    "scalar_metrics_df.columns = scalar_metrics\n",
    "scalar_metrics_df.reset_index(inplace=True)\n",
    "scalar_metrics_df[\"dataset\"] = scalar_metrics_df[\"dataset_algo\"].apply(lambda x: x.split(\"_\")[0])\n",
    "scalar_metrics_df[\"algo\"] = scalar_metrics_df[\"dataset_algo\"].apply(lambda x: x.split(\"_\")[1])\n",
    "scalar_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round all values to 3 decimal places\n",
    "scalar_metrics_df_to_save = scalar_metrics_df.copy()\n",
    "for metric in scalar_metrics:\n",
    "    scalar_metrics_df_to_save[metric] = scalar_metrics_df_to_save[metric].apply(lambda x: (round(x[0], 3), round(x[1], 3)))\n",
    "scalar_metrics_df_to_save.to_csv(\"results/scalar_metrics_sample60k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barwidth = 0.2\n",
    "n_datasets = len(scalar_metrics_df[\"dataset\"].unique())\n",
    "n_algos = len(scalar_metrics_df[\"algo\"].unique())\n",
    "colors = plt.cm.get_cmap(\"tab10\")\n",
    "legend_patches = [mpatches.Patch(color=colors(i), label=algo.upper()) for i, algo in enumerate(algo_names)]\n",
    "\n",
    "# Plot neighborhood preservation on single plot\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for i, dataset in enumerate(datasets_names):\n",
    "    for j, algo in enumerate(algo_names):\n",
    "        df = scalar_metrics_df[(scalar_metrics_df[\"dataset\"] == dataset) & (scalar_metrics_df[\"algo\"] == algo)]\n",
    "        mean,std = df[\"neighbor_kept_ratio\"].values[0]\n",
    "        ax.bar(i + j * barwidth, mean, yerr=std, width=barwidth, label=f\"{algo}\", color = colors(j))\n",
    "ax.set_xticks(np.arange(n_datasets) + barwidth * (n_algos - 1) / 2)\n",
    "ax.set_xticklabels([w.upper() for w in datasets_names], fontsize = 12, fontweight='bold')\n",
    "ax.set_ylabel(\"Neighborhood Kept Ratio\", fontsize = 12, fontweight='bold')\n",
    "ax.legend(handles=legend_patches, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot global score\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for i, dataset in enumerate(datasets_names):\n",
    "    for j, algo in enumerate(algo_names):\n",
    "        df = scalar_metrics_df[(scalar_metrics_df[\"dataset\"] == dataset) & (scalar_metrics_df[\"algo\"] == algo)]\n",
    "        mean,std = df[\"global_score\"].values[0]\n",
    "        ax.bar(i + j * barwidth, mean, yerr=std, width=barwidth, label=f\"{algo}\", color = colors(j))\n",
    "ax.set_xticks(np.arange(n_datasets) + barwidth * (n_algos - 1) / 2)\n",
    "ax.set_xticklabels([w.upper() for w in datasets_names], fontsize = 12, fontweight='bold')\n",
    "ax.set_ylabel(\"Global Score\", fontsize = 12, fontweight='bold')\n",
    "ax.legend(handles=legend_patches, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random triplet accuracy\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 8))\n",
    "for i, dataset in enumerate(datasets_names):\n",
    "    for j, algo in enumerate(algo_names):\n",
    "        df = scalar_metrics_df[(scalar_metrics_df[\"dataset\"] == dataset) & (scalar_metrics_df[\"algo\"] == algo)]\n",
    "        # Local RTA\n",
    "        mean,std = df[\"rta_local\"].values[0]\n",
    "        axs[0].bar(i + j * barwidth, mean, yerr=std, width=barwidth, label=f\"{algo}\", color = colors(j))\n",
    "        # Global RTA\n",
    "        mean,std = df[\"rta_global\"].values[0]\n",
    "        axs[1].bar(i + j * barwidth, mean, yerr=std, width=barwidth, label=f\"{algo}\", color = colors(j))\n",
    "        # All RTA\n",
    "        mean,std = df[\"rta_all\"].values[0]\n",
    "        axs[2].bar(i + j * barwidth, mean, yerr=std, width=barwidth, label=f\"{algo}\", color = colors(j))\n",
    "\n",
    "for ax,label in zip(axs,[\"Local\", \"Global\", \"All\"]):\n",
    "    ax.set_xticks(np.arange(n_datasets) + barwidth * (n_algos - 1) / 2)\n",
    "    ax.set_xticklabels([w.upper() for w in datasets_names], fontsize = 12, fontweight='bold')\n",
    "    ax.set_ylabel(label, fontsize = 12, fontweight='bold')\n",
    "plt.suptitle(\"Random Triplet Accuracy\", fontsize = 14, fontweight='bold')\n",
    "axs[0].legend(handles=legend_patches, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Random Triplet Accuracy (all)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for i, dataset in enumerate(datasets_names):\n",
    "    for j, algo in enumerate(algo_names):\n",
    "        df = scalar_metrics_df[(scalar_metrics_df[\"dataset\"] == dataset) & (scalar_metrics_df[\"algo\"] == algo)]\n",
    "        mean,std = df[\"rta_all\"].values[0]\n",
    "        ax.bar(i + j * barwidth, mean, yerr=std, width=barwidth, label=f\"{algo}\", color = colors(j))\n",
    "ax.set_xticks(np.arange(n_datasets) + barwidth * (n_algos - 1) / 2)\n",
    "ax.set_xticklabels([w.upper() for w in datasets_names], fontsize = 12, fontweight='bold')\n",
    "ax.set_ylabel(\"Random Triplet Accuracy\", fontsize = 12, fontweight='bold')\n",
    "ax.legend(handles=legend_patches, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Spearman correlation of distances\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for i, dataset in enumerate(datasets_names):\n",
    "    for j, algo in enumerate(algo_names):\n",
    "        df = scalar_metrics_df[(scalar_metrics_df[\"dataset\"] == dataset) & (scalar_metrics_df[\"algo\"] == algo)]\n",
    "        mean,std = df[\"dist_corr\"].values[0]\n",
    "        ax.bar(i + j * barwidth, np.abs(mean), yerr=std, width=barwidth, label=f\"{algo}\", color = colors(j))\n",
    "ax.set_xticks(np.arange(n_datasets) + barwidth * (n_algos - 1) / 2)\n",
    "ax.set_xticklabels([w.upper() for w in datasets_names], fontsize = 12, fontweight='bold')\n",
    "ax.set_ylabel(\"Distances Spearman Correlation\", fontsize = 12, fontweight='bold')\n",
    "ax.legend(handles=legend_patches, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single plot for all scalar metrics, in a column\n",
    "metrics_to_plot = [\"neighbor_kept_ratio\", \"global_score\", \"rta_all\", \"dist_corr\"]\n",
    "metrics_labels = [\"Neighborhood Kept Ratio\", \"Global Score\", \"Random Triplet Accuracy\", \"Distances Spearman Correlation\"]\n",
    "fig, axs = plt.subplots(len(metrics_to_plot) + 1, 1, figsize=(6, 10))\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    for j, dataset in enumerate(datasets_names):\n",
    "        for k, algo in enumerate(algo_names):\n",
    "            df = scalar_metrics_df[(scalar_metrics_df[\"dataset\"] == dataset) & (scalar_metrics_df[\"algo\"] == algo)]\n",
    "            mean,std = df[metric].values[0]\n",
    "            axs[i+1].bar(j + k * barwidth, mean, yerr=std, width=barwidth, label=f\"{algo}\", color = colors(k))\n",
    "    axs[i+1].set_xticks(np.arange(n_datasets) + barwidth * (n_algos - 1) / 2)\n",
    "    axs[i+1].set_xticklabels([w.upper() for w in datasets_names], fontsize = 12, fontweight='bold')\n",
    "    axs[i+1].set_ylabel(tw.fill(metrics_labels[i], 15), fontsize = 12, fontweight='bold')\n",
    "plt.suptitle(\"Scalar Metrics\", fontsize = 14, fontweight='bold')\n",
    "axs[0].legend(handles=legend_patches, loc=\"lower left\", fontsize = 14)\n",
    "axs[0].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics at rank k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_at_k = [\"t_values\", \"c_values\", \"lcmc_values\"]\n",
    "at_k_metrics = evaluation_results_df[[\"dataset_algo\"] + metrics_at_k]\n",
    "at_k_metrics[\"dataset\"] = at_k_metrics[\"dataset_algo\"].apply(lambda x: x.split(\"_\")[0])\n",
    "at_k_metrics[\"algo\"] = at_k_metrics[\"dataset_algo\"].apply(lambda x: x.split(\"_\")[1])\n",
    "at_k_metrics[\"run\"] = at_k_metrics[\"dataset_algo\"].apply(lambda x: x.split(\"_\")[2].replace(\"run\", \"\")).astype(int)\n",
    "\n",
    "at_k_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = np.arange(0,len(at_k_metrics[\"t_values\"].values[0]))\n",
    "k_metrics_handles = []\n",
    "k_metrics_colors = plt.cm.get_cmap(\"tab20\", len(algo_names) * len(datasets_names))\n",
    "\n",
    "fig, axs = plt.subplots(3, len(datasets_names), figsize=(16, 8))\n",
    "\n",
    "for d,dataset in enumerate(datasets_names):\n",
    "    for i,algo in enumerate(algo_names):\n",
    "        k_metrics_handles.append(f\"{dataset}_{algo}\")\n",
    "        df = at_k_metrics[(at_k_metrics[\"dataset\"] == dataset) & (at_k_metrics[\"algo\"] == algo)]\n",
    "        t_values = df[\"t_values\"].values\n",
    "        c_values = df[\"c_values\"].values\n",
    "        lcmc_values = df[\"lcmc_values\"].values\n",
    "        for j in range(len(t_values)):\n",
    "            axs[0,d].scatter(k_values, t_values[j], color=colors(i), linestyle='-', marker='x', s=1)\n",
    "            axs[1,d].scatter(k_values, c_values[j], color=colors(i), linestyle='-', marker='x', s=1)\n",
    "            axs[2,d].scatter(k_values, lcmc_values[j], color=colors(i), linestyle='-', marker='x', s=1)\n",
    "        \n",
    "plt.suptitle(\"Metrics at K\", fontsize = 14, fontweight='bold')\n",
    "axs[0,0].set_ylabel(\"Trustworthiness\", fontsize = 12, fontweight='bold')\n",
    "axs[1,0].set_ylabel(\"Continuity\", fontsize = 12, fontweight='bold')\n",
    "axs[2,0].set_ylabel(\"LCMC\", fontsize = 12, fontweight='bold')\n",
    "# Shared x axis labels\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlabel(\"K\", fontsize = 12, fontweight='bold')\n",
    "\n",
    "axs[0,3].legend(handles=legend_patches, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr_map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
